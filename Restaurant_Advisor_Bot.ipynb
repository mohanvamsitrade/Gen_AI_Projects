{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDrglMrnq+pMRZcSs7Fcic",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohanvamsitrade/Gen_AI_Projects/blob/main/Restaurant_Advisor_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly Importing and Installing few Libraries\n",
        "\n",
        "1. Obtain a API key from Huggingface/OpenAI (Had some issues with OPENAI API, So continuing with Huggingface)\n",
        "2. Installing Langchain Framework\n",
        "3. Installing tiktoken and cohere (You may need these incase of using Hugginface, If using OpenAI model you may ignore unless OPENAI installation throws any error)\n",
        "4. Install Huggingface or OPENAI depending on the LLM model which you are using\n"
      ],
      "metadata": {
        "id": "k-sUA7eIgi0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJLfrV8m4DWU"
      },
      "outputs": [],
      "source": [
        "from secret_key import huggingface_key1\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = huggingface_key1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "IzVJZHuU4lh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "mBWWoj0u5Z0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere\n"
      ],
      "metadata": {
        "id": "9P00Fz_E44HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade typing-extensions==4.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-3Z8bBk5kgr",
        "outputId": "72eb6c19-0cab-4f6d-c4bb-200706cd57b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: typing-extensions==4.7 in /usr/local/lib/python3.10/dist-packages (4.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade llmx"
      ],
      "metadata": {
        "id": "fRhrzMxv54uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFo2esOF90E7",
        "outputId": "dfb69304-d450-4f4f-97c4-b0490d33d9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface in /usr/local/lib/python3.10/dist-packages (0.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain huggingface_hub transformers sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmJrq6NNA3Xk",
        "outputId": "41253555-de22-41a1-82aa-3ffdaf461af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/86.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain is key framework needed which working with LLM's.. Its lot easier with Lanchain trust me\n",
        "\n",
        "1. Import HuggingFaceHub from LangChain\n",
        "2. Initiating HuggingFaceHub into a variable llm\n",
        "3. I have considered a model \"google/flan-t5-xxl\" from huggingface, you can use other models in (Seq2Seq).. For now lets go with this model..\n",
        "4. Incase of having OpenAI API key, you can continiue with OPENAI model..As per my comparision for the below program OPENAI works much better than the FLAN-T5 which i have considered"
      ],
      "metadata": {
        "id": "HhTSc-cCiHRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceHub(repo_id = \"google/flan-t5-xxl\", model_kwargs= {'temperature':0.6})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rKMEYcj6Dd-",
        "outputId": "6f870813-055a-4772-e5ef-b7229abab2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above few steps are good enough to start using the model which we selected\n",
        "\n",
        "1. A simple prompt given into the model yields result just like Chat GPT"
      ],
      "metadata": {
        "id": "TpSXa3vcikf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = llm(\"I want to open a restaurant for Indian food. Suggest a fency name for this\")\n",
        "print(name)\n",
        "\n",
        "menu = llm(\"Give me a 5 menu names for Indian restaurant\")\n",
        "print(menu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLcSrv5L6J74",
        "outputId": "7e73ae5e-1049-402a-b388-a93602f0d4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indian Fusion\n",
            "tandoori chicken tikka nan sag aloo dhaniya korma saag paneer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Templates are the next important modules in Langchain which help you in giving multiple prompts without repeting much of the sentences\n",
        "\n",
        "1. Below i used PromptTemplate to customize a prompt with a certain input variable of my interest so that i can make changes just to the variable and acquire different results while using the model\n",
        "2. Below i a trying to extract some fancy restaurant names from the model based on the cuisine of my interest\n",
        "3. I have also created another prompt which queiries my model for some menu items based on the restaurant name give"
      ],
      "metadata": {
        "id": "htC-nbHPjw7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name  = PromptTemplate(\n",
        "    input_variables=['cusine'], template = \"I want to open a restaurant for {cusine} food. Suggest a fency name for this\"\n",
        ")\n",
        "\n",
        "prompt_template_name.format(cusine = 'English')\n",
        "\n",
        "prompt_template_menu = PromptTemplate(\n",
        "    input_variables=['restaurant_name'],\n",
        "    template=\"Suggest 6 menu items for {restaurant_name}. Return it as a comma separated list\"\n",
        ")"
      ],
      "metadata": {
        "id": "Klh1qugMIGJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMChains help you in using both the model and the Prompttemplate under a signal umberella\n",
        "\n",
        "1. Below i have created \"name_chain\" and a \"menu_chain\""
      ],
      "metadata": {
        "id": "2ZR6q0ZolAfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "name_chain = LLMChain(llm = llm, prompt= prompt_template_name)\n",
        "menu_chain = LLMChain(llm = llm, prompt= prompt_template_menu)\n",
        "name_chain.invoke('Indian')\n",
        "menu_chain.invoke('Indian')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf_xr96FI_F1",
        "outputId": "484edacf-730d-4cdf-cc36-e8973b5edfc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'restaurant_name': 'Indian', 'text': ',,,, '}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "chain = SimpleSequentialChain(chains=[name_chain, menu_chain])\n",
        "\n",
        "response = chain.invoke(\"Mexican\")\n",
        "print(response)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6uvQuXUKJ9E",
        "outputId": "4085d557-eb49-4668-9dc4-363b36da63ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': 'Mexican', 'output': 'fajitas, enchiladas, burrito, carnitas, chile relleno'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets use all the knowledge we have gained earlier\n",
        "\n",
        "1. First creating 2 prompts which we provide \"Cusine\" and \"resturant_name\" as variables to the prompts individually\n",
        "2. creating LLMChain's using the prompts, we will be using the same LLM model \"google/flan-t5-xxl\" as earlier.. No changes to the model made"
      ],
      "metadata": {
        "id": "e6Xxb-3hlnEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_name  = PromptTemplate(\n",
        "    input_variables=['cusine'],\n",
        "    template = \"I want to open a restaurant for {cusine} food. Suggest a fency name for this\"\n",
        ")\n",
        "\n",
        "prompt_template_menu = PromptTemplate(\n",
        "    input_variables=['restaurant_name'],\n",
        "    template=\"Suggest 6 menu items for {restaurant_name}. Return it as a comma separated list\"\n",
        ")\n",
        "\n",
        "name_chain = LLMChain(llm = llm, prompt= prompt_template_name,output_key=\"restaurant_name\")\n",
        "menu_chain = LLMChain(llm = llm, prompt= prompt_template_menu, output_key= \"menu_items\")"
      ],
      "metadata": {
        "id": "hunMsH9vNDdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are upgrading ourselves here.. We are going to link the 2 LLMChian's which we created earlier\n",
        "\n",
        "Simply to say, the output of the first LLMChain acts as the input for the 2nd LLMChain\n",
        "\n",
        "1. Lets import SequentialChain from Langchain.chains (You might have already understood the significance of SequentialChain module here.. Using LLMChains in Sequentially)\n",
        "2. So our first LLM Chain is \"name_chain\" which takes input \"Cuisine\" and gives output \"restautant_name\"\n",
        "3. Now the input for the sencond LLM Chain is \"restaurant_name\" and gives \"menu_items\" as output\n",
        "4. Using using both chains in our SequentialChain module gives considers \"cuisine\" as input variable and provides output of \"restaurant_name\" and \"menu_items\" as output variable\n",
        "\n",
        "Great Work guys.. If you have come till here.. You have made your first step in using GENAI as per your interest\n",
        "\n",
        "Now lets create a short app using streamlit using this simple model"
      ],
      "metadata": {
        "id": "5Kh23QWlmJZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "new_chain = SequentialChain(\n",
        "    chains = [name_chain, menu_chain],\n",
        "    input_variables=['cusine'],\n",
        "    output_variables= ['restaurant_name','menu_items']\n",
        ")\n",
        "\n",
        "new_chain({'cusine':'Mexican'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymj0l25FN7IX",
        "outputId": "849ff643-cc66-494f-98a0-dc4ef57b5233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cusine': 'Mexican',\n",
              " 'restaurant_name': 'Mexican food restaurant',\n",
              " 'menu_items': 'fajitas, enchiladas, burrito, carnitas, chile relleno'}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I753XynbOmtC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}